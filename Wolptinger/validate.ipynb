{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd084b686020625b0e4c68be0b0fcd7ab3251cfecf2e3c65a384757419db216fdad",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "import gym\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from normalised_env import NormalizedEnv\n",
    "from wolptinger import Wolptinger\n",
    "from utils import *\n",
    "from evaluator import Evaluator\n",
    "from ContinuousCartPole import ContinuousCartPoleEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_iterations, agent, env, evaluate, validate_steps, output, max_episode_length=None, debug=False):\n",
    "    agent.is_training = True\n",
    "    step = episode = episode_steps = 0\n",
    "    episode_reward = 0.\n",
    "    observation = None\n",
    "\n",
    "    while step < num_iterations:\n",
    "        # reset if it is the start of episode\n",
    "        if observation is None:\n",
    "            # the original method use deepcopy here\n",
    "            observation = env.reset()\n",
    "            agent.reset(observation)\n",
    "        \n",
    "        # agent choose action\n",
    "        if step < args.warmup:\n",
    "            action = agent.random_action()\n",
    "        else:\n",
    "            action = agent.select_action(observation)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if max_episode_length and episode_steps >= max_episode_length - 1:\n",
    "            done = True\n",
    "        \n",
    "        # agent observe and update policy\n",
    "        agent.observe(reward, next_state, done)\n",
    "        if step > args.warmup:\n",
    "            agent.update_policy()\n",
    "        \n",
    "        # evaluate the model\n",
    "        # I personally don't think this will work, also, this will let the model memory a wrong\n",
    "        # action\n",
    "        if evaluate is not None and validate_steps > 0 and step % validate_steps == 0:\n",
    "            policy = lambda x: agent.select_action(x, decay_epsilon=False)\n",
    "            validate_reward = evaluate(env, policy, debug=False, visualize=False, save=False)\n",
    "            if debug: \n",
    "                prYellow('[Evaluate] Step_{:07d}: mean_reward:{}'.format(step, validate_reward))\n",
    "            agent.s_t = observation\n",
    "            agent.a_t = action\n",
    "\n",
    "        # save intermidate training model\n",
    "        if step % int(num_iterations/3) == 0:\n",
    "            agent.save_model(output)\n",
    "        \n",
    "        # update models\n",
    "        step += 1\n",
    "        episode_steps += 1\n",
    "        episode_reward += reward\n",
    "        observation = next_state\n",
    "        \n",
    "        # end of episode\n",
    "        if done:\n",
    "            if debug:\n",
    "                prLightPurple(f'#{episode}: episode_reward: {episode_reward} steps:{step}')\n",
    "            \n",
    "            # agent.memory.append(\n",
    "            #     observation,\n",
    "            #     agent.select_action(observation)\n",
    "            #     0.,\n",
    "            #     False\n",
    "            # )\n",
    "\n",
    "            # reset\n",
    "            observation = None\n",
    "            episode_steps = 0\n",
    "            episode_reward = 0.\n",
    "            episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(num_episodes, agent, env, evaluate, model_path, visualize=True, debug=False):\n",
    "\n",
    "    agent.load_weights(model_path)\n",
    "    agent.is_training = False\n",
    "    agent.eval()\n",
    "    policy = lambda x: agent.select_action(x, decay_epsilon=False)\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        validate_reward = evaluate(env, policy, debug=debug, visualize=visualize, save=False)\n",
    "        if debug: \n",
    "            prYellow('[Evaluate] #{}: mean_reward:{}'.format(i, validate_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments(object):\n",
    "    def __init__(self):\n",
    "        self.mode = 'train'\n",
    "        # self.env = \"InvertedPendulum-v2\"\n",
    "        self.env = 'ContinuousCartPole'\n",
    "        self.h1 = 128\n",
    "        self.h2 = 128\n",
    "        self.rate = 1e-3\n",
    "        self.prate = 1e-3\n",
    "        self.warmup = 100\n",
    "        self.discount = 0.99\n",
    "        self.bsize = 64\n",
    "        self.rmsize = 1000\n",
    "        self.window_length = 1\n",
    "        self.tau = 0.001\n",
    "        self.ou_theta = 0.15\n",
    "        self.ou_sigma = 0.2\n",
    "        self.ou_mu = 0.0\n",
    "        self.validate_episodes = 20\n",
    "        self.max_episode_length = 500\n",
    "        self.validate_steps = 2000\n",
    "        self.output = 'output'\n",
    "        self.debug = True\n",
    "        self.init_w = 0.003\n",
    "        self.train_iter = 20000\n",
    "        # self.epsilon = 50000\n",
    "        self.epsilon = 10000\n",
    "        self.seed = -1\n",
    "        self.max_actions = 1e6\n",
    "        self.resume = 'default'\n",
    "        self.k_ratio = 1e-6\n",
    "        # we pass in the pre_generated action space\n",
    "        self.action_space = None\n",
    "\n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "rd: 6.0 steps:8969\u001b[00m\n",
      "\u001b[94m #1524: episode_reward: 6.0 steps:8975\u001b[00m\n",
      "\u001b[94m #1525: episode_reward: 5.0 steps:8980\u001b[00m\n",
      "\u001b[94m #1526: episode_reward: 6.0 steps:8986\u001b[00m\n",
      "\u001b[94m #1527: episode_reward: 5.0 steps:8991\u001b[00m\n",
      "\u001b[94m #1528: episode_reward: 6.0 steps:8997\u001b[00m\n",
      "\u001b[94m #1529: episode_reward: 5.0 steps:9002\u001b[00m\n",
      "\u001b[94m #1530: episode_reward: 6.0 steps:9008\u001b[00m\n",
      "\u001b[94m #1531: episode_reward: 6.0 steps:9014\u001b[00m\n",
      "\u001b[94m #1532: episode_reward: 6.0 steps:9020\u001b[00m\n",
      "\u001b[94m #1533: episode_reward: 6.0 steps:9026\u001b[00m\n",
      "\u001b[94m #1534: episode_reward: 6.0 steps:9032\u001b[00m\n",
      "\u001b[94m #1535: episode_reward: 5.0 steps:9037\u001b[00m\n",
      "\u001b[94m #1536: episode_reward: 6.0 steps:9043\u001b[00m\n",
      "\u001b[94m #1537: episode_reward: 6.0 steps:9049\u001b[00m\n",
      "\u001b[94m #1538: episode_reward: 6.0 steps:9055\u001b[00m\n",
      "\u001b[94m #1539: episode_reward: 6.0 steps:9061\u001b[00m\n",
      "\u001b[94m #1540: episode_reward: 6.0 steps:9067\u001b[00m\n",
      "\u001b[94m #1541: episode_reward: 5.0 steps:9072\u001b[00m\n",
      "\u001b[94m #1542: episode_reward: 6.0 steps:9078\u001b[00m\n",
      "\u001b[94m #1543: episode_reward: 6.0 steps:9084\u001b[00m\n",
      "\u001b[94m #1544: episode_reward: 6.0 steps:9090\u001b[00m\n",
      "\u001b[94m #1545: episode_reward: 6.0 steps:9096\u001b[00m\n",
      "\u001b[94m #1546: episode_reward: 6.0 steps:9102\u001b[00m\n",
      "\u001b[94m #1547: episode_reward: 6.0 steps:9108\u001b[00m\n",
      "\u001b[94m #1548: episode_reward: 6.0 steps:9114\u001b[00m\n",
      "\u001b[94m #1549: episode_reward: 6.0 steps:9120\u001b[00m\n",
      "\u001b[94m #1550: episode_reward: 6.0 steps:9126\u001b[00m\n",
      "\u001b[94m #1551: episode_reward: 6.0 steps:9132\u001b[00m\n",
      "\u001b[94m #1552: episode_reward: 5.0 steps:9137\u001b[00m\n",
      "\u001b[94m #1553: episode_reward: 6.0 steps:9143\u001b[00m\n",
      "\u001b[94m #1554: episode_reward: 6.0 steps:9149\u001b[00m\n",
      "\u001b[94m #1555: episode_reward: 7.0 steps:9156\u001b[00m\n",
      "\u001b[94m #1556: episode_reward: 5.0 steps:9161\u001b[00m\n",
      "\u001b[94m #1557: episode_reward: 6.0 steps:9167\u001b[00m\n",
      "\u001b[94m #1558: episode_reward: 6.0 steps:9173\u001b[00m\n",
      "\u001b[94m #1559: episode_reward: 6.0 steps:9179\u001b[00m\n",
      "\u001b[94m #1560: episode_reward: 6.0 steps:9185\u001b[00m\n",
      "\u001b[94m #1561: episode_reward: 6.0 steps:9191\u001b[00m\n",
      "\u001b[94m #1562: episode_reward: 5.0 steps:9196\u001b[00m\n",
      "\u001b[94m #1563: episode_reward: 6.0 steps:9202\u001b[00m\n",
      "\u001b[94m #1564: episode_reward: 6.0 steps:9208\u001b[00m\n",
      "\u001b[94m #1565: episode_reward: 6.0 steps:9214\u001b[00m\n",
      "\u001b[94m #1566: episode_reward: 6.0 steps:9220\u001b[00m\n",
      "\u001b[94m #1567: episode_reward: 6.0 steps:9226\u001b[00m\n",
      "\u001b[94m #1568: episode_reward: 6.0 steps:9232\u001b[00m\n",
      "\u001b[94m #1569: episode_reward: 6.0 steps:9238\u001b[00m\n",
      "\u001b[94m #1570: episode_reward: 6.0 steps:9244\u001b[00m\n",
      "\u001b[94m #1571: episode_reward: 6.0 steps:9250\u001b[00m\n",
      "\u001b[94m #1572: episode_reward: 6.0 steps:9256\u001b[00m\n",
      "\u001b[94m #1573: episode_reward: 6.0 steps:9262\u001b[00m\n",
      "\u001b[94m #1574: episode_reward: 5.0 steps:9267\u001b[00m\n",
      "\u001b[94m #1575: episode_reward: 6.0 steps:9273\u001b[00m\n",
      "\u001b[94m #1576: episode_reward: 6.0 steps:9279\u001b[00m\n",
      "\u001b[94m #1577: episode_reward: 6.0 steps:9285\u001b[00m\n",
      "\u001b[94m #1578: episode_reward: 6.0 steps:9291\u001b[00m\n",
      "\u001b[94m #1579: episode_reward: 5.0 steps:9296\u001b[00m\n",
      "\u001b[94m #1580: episode_reward: 6.0 steps:9302\u001b[00m\n",
      "\u001b[94m #1581: episode_reward: 6.0 steps:9308\u001b[00m\n",
      "\u001b[94m #1582: episode_reward: 6.0 steps:9314\u001b[00m\n",
      "\u001b[94m #1583: episode_reward: 6.0 steps:9320\u001b[00m\n",
      "\u001b[94m #1584: episode_reward: 6.0 steps:9326\u001b[00m\n",
      "\u001b[94m #1585: episode_reward: 6.0 steps:9332\u001b[00m\n",
      "\u001b[94m #1586: episode_reward: 6.0 steps:9338\u001b[00m\n",
      "\u001b[94m #1587: episode_reward: 6.0 steps:9344\u001b[00m\n",
      "\u001b[94m #1588: episode_reward: 6.0 steps:9350\u001b[00m\n",
      "\u001b[94m #1589: episode_reward: 6.0 steps:9356\u001b[00m\n",
      "\u001b[94m #1590: episode_reward: 6.0 steps:9362\u001b[00m\n",
      "\u001b[94m #1591: episode_reward: 6.0 steps:9368\u001b[00m\n",
      "\u001b[94m #1592: episode_reward: 6.0 steps:9374\u001b[00m\n",
      "\u001b[94m #1593: episode_reward: 6.0 steps:9380\u001b[00m\n",
      "\u001b[94m #1594: episode_reward: 6.0 steps:9386\u001b[00m\n",
      "\u001b[94m #1595: episode_reward: 6.0 steps:9392\u001b[00m\n",
      "\u001b[94m #1596: episode_reward: 6.0 steps:9398\u001b[00m\n",
      "\u001b[94m #1597: episode_reward: 6.0 steps:9404\u001b[00m\n",
      "\u001b[94m #1598: episode_reward: 6.0 steps:9410\u001b[00m\n",
      "\u001b[94m #1599: episode_reward: 6.0 steps:9416\u001b[00m\n",
      "\u001b[94m #1600: episode_reward: 5.0 steps:9421\u001b[00m\n",
      "\u001b[94m #1601: episode_reward: 6.0 steps:9427\u001b[00m\n",
      "\u001b[94m #1602: episode_reward: 5.0 steps:9432\u001b[00m\n",
      "\u001b[94m #1603: episode_reward: 6.0 steps:9438\u001b[00m\n",
      "\u001b[94m #1604: episode_reward: 6.0 steps:9444\u001b[00m\n",
      "\u001b[94m #1605: episode_reward: 6.0 steps:9450\u001b[00m\n",
      "\u001b[94m #1606: episode_reward: 5.0 steps:9455\u001b[00m\n",
      "\u001b[94m #1607: episode_reward: 6.0 steps:9461\u001b[00m\n",
      "\u001b[94m #1608: episode_reward: 5.0 steps:9466\u001b[00m\n",
      "\u001b[94m #1609: episode_reward: 5.0 steps:9471\u001b[00m\n",
      "\u001b[94m #1610: episode_reward: 6.0 steps:9477\u001b[00m\n",
      "\u001b[94m #1611: episode_reward: 6.0 steps:9483\u001b[00m\n",
      "\u001b[94m #1612: episode_reward: 6.0 steps:9489\u001b[00m\n",
      "\u001b[94m #1613: episode_reward: 6.0 steps:9495\u001b[00m\n",
      "\u001b[94m #1614: episode_reward: 6.0 steps:9501\u001b[00m\n",
      "\u001b[94m #1615: episode_reward: 6.0 steps:9507\u001b[00m\n",
      "\u001b[94m #1616: episode_reward: 5.0 steps:9512\u001b[00m\n",
      "\u001b[94m #1617: episode_reward: 6.0 steps:9518\u001b[00m\n",
      "\u001b[94m #1618: episode_reward: 5.0 steps:9523\u001b[00m\n",
      "\u001b[94m #1619: episode_reward: 6.0 steps:9529\u001b[00m\n",
      "\u001b[94m #1620: episode_reward: 5.0 steps:9534\u001b[00m\n",
      "\u001b[94m #1621: episode_reward: 6.0 steps:9540\u001b[00m\n",
      "\u001b[94m #1622: episode_reward: 6.0 steps:9546\u001b[00m\n",
      "\u001b[94m #1623: episode_reward: 5.0 steps:9551\u001b[00m\n",
      "\u001b[94m #1624: episode_reward: 5.0 steps:9556\u001b[00m\n",
      "\u001b[94m #1625: episode_reward: 6.0 steps:9562\u001b[00m\n",
      "\u001b[94m #1626: episode_reward: 6.0 steps:9568\u001b[00m\n",
      "\u001b[94m #1627: episode_reward: 6.0 steps:9574\u001b[00m\n",
      "\u001b[94m #1628: episode_reward: 6.0 steps:9580\u001b[00m\n",
      "\u001b[94m #1629: episode_reward: 5.0 steps:9585\u001b[00m\n",
      "\u001b[94m #1630: episode_reward: 6.0 steps:9591\u001b[00m\n",
      "\u001b[94m #1631: episode_reward: 5.0 steps:9596\u001b[00m\n",
      "\u001b[94m #1632: episode_reward: 6.0 steps:9602\u001b[00m\n",
      "\u001b[94m #1633: episode_reward: 6.0 steps:9608\u001b[00m\n",
      "\u001b[94m #1634: episode_reward: 6.0 steps:9614\u001b[00m\n",
      "\u001b[94m #1635: episode_reward: 5.0 steps:9619\u001b[00m\n",
      "\u001b[94m #1636: episode_reward: 6.0 steps:9625\u001b[00m\n",
      "\u001b[94m #1637: episode_reward: 5.0 steps:9630\u001b[00m\n",
      "\u001b[94m #1638: episode_reward: 6.0 steps:9636\u001b[00m\n",
      "\u001b[94m #1639: episode_reward: 7.0 steps:9643\u001b[00m\n",
      "\u001b[94m #1640: episode_reward: 5.0 steps:9648\u001b[00m\n",
      "\u001b[94m #1641: episode_reward: 5.0 steps:9653\u001b[00m\n",
      "\u001b[94m #1642: episode_reward: 5.0 steps:9658\u001b[00m\n",
      "\u001b[94m #1643: episode_reward: 6.0 steps:9664\u001b[00m\n",
      "\u001b[94m #1644: episode_reward: 6.0 steps:9670\u001b[00m\n",
      "\u001b[94m #1645: episode_reward: 6.0 steps:9676\u001b[00m\n",
      "\u001b[94m #1646: episode_reward: 6.0 steps:9682\u001b[00m\n",
      "\u001b[94m #1647: episode_reward: 6.0 steps:9688\u001b[00m\n",
      "\u001b[94m #1648: episode_reward: 6.0 steps:9694\u001b[00m\n",
      "\u001b[94m #1649: episode_reward: 6.0 steps:9700\u001b[00m\n",
      "\u001b[94m #1650: episode_reward: 6.0 steps:9706\u001b[00m\n",
      "\u001b[94m #1651: episode_reward: 6.0 steps:9712\u001b[00m\n",
      "\u001b[94m #1652: episode_reward: 6.0 steps:9718\u001b[00m\n",
      "\u001b[94m #1653: episode_reward: 6.0 steps:9724\u001b[00m\n",
      "\u001b[94m #1654: episode_reward: 5.0 steps:9729\u001b[00m\n",
      "\u001b[94m #1655: episode_reward: 6.0 steps:9735\u001b[00m\n",
      "\u001b[94m #1656: episode_reward: 5.0 steps:9740\u001b[00m\n",
      "\u001b[94m #1657: episode_reward: 6.0 steps:9746\u001b[00m\n",
      "\u001b[94m #1658: episode_reward: 6.0 steps:9752\u001b[00m\n",
      "\u001b[94m #1659: episode_reward: 6.0 steps:9758\u001b[00m\n",
      "\u001b[94m #1660: episode_reward: 6.0 steps:9764\u001b[00m\n",
      "\u001b[94m #1661: episode_reward: 6.0 steps:9770\u001b[00m\n",
      "\u001b[94m #1662: episode_reward: 6.0 steps:9776\u001b[00m\n",
      "\u001b[94m #1663: episode_reward: 6.0 steps:9782\u001b[00m\n",
      "\u001b[94m #1664: episode_reward: 6.0 steps:9788\u001b[00m\n",
      "\u001b[94m #1665: episode_reward: 6.0 steps:9794\u001b[00m\n",
      "\u001b[94m #1666: episode_reward: 6.0 steps:9800\u001b[00m\n",
      "\u001b[94m #1667: episode_reward: 6.0 steps:9806\u001b[00m\n",
      "\u001b[94m #1668: episode_reward: 6.0 steps:9812\u001b[00m\n",
      "\u001b[94m #1669: episode_reward: 5.0 steps:9817\u001b[00m\n",
      "\u001b[94m #1670: episode_reward: 6.0 steps:9823\u001b[00m\n",
      "\u001b[94m #1671: episode_reward: 6.0 steps:9829\u001b[00m\n",
      "\u001b[94m #1672: episode_reward: 6.0 steps:9835\u001b[00m\n",
      "\u001b[94m #1673: episode_reward: 6.0 steps:9841\u001b[00m\n",
      "\u001b[94m #1674: episode_reward: 6.0 steps:9847\u001b[00m\n",
      "\u001b[94m #1675: episode_reward: 6.0 steps:9853\u001b[00m\n",
      "\u001b[94m #1676: episode_reward: 6.0 steps:9859\u001b[00m\n",
      "\u001b[94m #1677: episode_reward: 6.0 steps:9865\u001b[00m\n",
      "\u001b[94m #1678: episode_reward: 6.0 steps:9871\u001b[00m\n",
      "\u001b[94m #1679: episode_reward: 5.0 steps:9876\u001b[00m\n",
      "\u001b[94m #1680: episode_reward: 6.0 steps:9882\u001b[00m\n",
      "\u001b[94m #1681: episode_reward: 6.0 steps:9888\u001b[00m\n",
      "\u001b[94m #1682: episode_reward: 6.0 steps:9894\u001b[00m\n",
      "\u001b[94m #1683: episode_reward: 6.0 steps:9900\u001b[00m\n",
      "\u001b[94m #1684: episode_reward: 6.0 steps:9906\u001b[00m\n",
      "\u001b[94m #1685: episode_reward: 6.0 steps:9912\u001b[00m\n",
      "\u001b[94m #1686: episode_reward: 5.0 steps:9917\u001b[00m\n",
      "\u001b[94m #1687: episode_reward: 6.0 steps:9923\u001b[00m\n",
      "\u001b[94m #1688: episode_reward: 6.0 steps:9929\u001b[00m\n",
      "\u001b[94m #1689: episode_reward: 5.0 steps:9934\u001b[00m\n",
      "\u001b[94m #1690: episode_reward: 6.0 steps:9940\u001b[00m\n",
      "\u001b[94m #1691: episode_reward: 6.0 steps:9946\u001b[00m\n",
      "\u001b[94m #1692: episode_reward: 6.0 steps:9952\u001b[00m\n",
      "\u001b[94m #1693: episode_reward: 6.0 steps:9958\u001b[00m\n",
      "\u001b[94m #1694: episode_reward: 5.0 steps:9963\u001b[00m\n",
      "\u001b[94m #1695: episode_reward: 6.0 steps:9969\u001b[00m\n",
      "\u001b[94m #1696: episode_reward: 6.0 steps:9975\u001b[00m\n",
      "\u001b[94m #1697: episode_reward: 6.0 steps:9981\u001b[00m\n",
      "\u001b[94m #1698: episode_reward: 6.0 steps:9987\u001b[00m\n",
      "\u001b[94m #1699: episode_reward: 6.0 steps:9993\u001b[00m\n",
      "\u001b[94m #1700: episode_reward: 6.0 steps:9999\u001b[00m\n",
      "\u001b[93m [Evaluate] Step_0010000: mean_reward:5.75\u001b[00m\n",
      "\u001b[94m #1701: episode_reward: 2.0 steps:10002\u001b[00m\n",
      "\u001b[94m #1702: episode_reward: 6.0 steps:10008\u001b[00m\n",
      "\u001b[94m #1703: episode_reward: 6.0 steps:10014\u001b[00m\n",
      "\u001b[94m #1704: episode_reward: 6.0 steps:10020\u001b[00m\n",
      "\u001b[94m #1705: episode_reward: 6.0 steps:10026\u001b[00m\n",
      "\u001b[94m #1706: episode_reward: 6.0 steps:10032\u001b[00m\n",
      "\u001b[94m #1707: episode_reward: 5.0 steps:10037\u001b[00m\n",
      "\u001b[94m #1708: episode_reward: 6.0 steps:10043\u001b[00m\n",
      "\u001b[94m #1709: episode_reward: 6.0 steps:10049\u001b[00m\n",
      "\u001b[94m #1710: episode_reward: 5.0 steps:10054\u001b[00m\n",
      "\u001b[94m #1711: episode_reward: 6.0 steps:10060\u001b[00m\n",
      "\u001b[94m #1712: episode_reward: 6.0 steps:10066\u001b[00m\n",
      "\u001b[94m #1713: episode_reward: 6.0 steps:10072\u001b[00m\n",
      "\u001b[94m #1714: episode_reward: 7.0 steps:10079\u001b[00m\n",
      "\u001b[94m #1715: episode_reward: 6.0 steps:10085\u001b[00m\n",
      "\u001b[94m #1716: episode_reward: 6.0 steps:10091\u001b[00m\n",
      "\u001b[94m #1717: episode_reward: 5.0 steps:10096\u001b[00m\n",
      "\u001b[94m #1718: episode_reward: 6.0 steps:10102\u001b[00m\n",
      "\u001b[94m #1719: episode_reward: 6.0 steps:10108\u001b[00m\n",
      "\u001b[94m #1720: episode_reward: 6.0 steps:10114\u001b[00m\n",
      "\u001b[94m #1721: episode_reward: 6.0 steps:10120\u001b[00m\n",
      "\u001b[94m #1722: episode_reward: 6.0 steps:10126\u001b[00m\n",
      "\u001b[94m #1723: episode_reward: 6.0 steps:10132\u001b[00m\n",
      "\u001b[94m #1724: episode_reward: 5.0 steps:10137\u001b[00m\n",
      "\u001b[94m #1725: episode_reward: 5.0 steps:10142\u001b[00m\n",
      "\u001b[94m #1726: episode_reward: 6.0 steps:10148\u001b[00m\n",
      "\u001b[94m #1727: episode_reward: 6.0 steps:10154\u001b[00m\n",
      "\u001b[94m #1728: episode_reward: 6.0 steps:10160\u001b[00m\n",
      "\u001b[94m #1729: episode_reward: 6.0 steps:10166\u001b[00m\n",
      "\u001b[94m #1730: episode_reward: 6.0 steps:10172\u001b[00m\n",
      "\u001b[94m #1731: episode_reward: 6.0 steps:10178\u001b[00m\n",
      "\u001b[94m #1732: episode_reward: 6.0 steps:10184\u001b[00m\n",
      "\u001b[94m #1733: episode_reward: 5.0 steps:10189\u001b[00m\n",
      "\u001b[94m #1734: episode_reward: 6.0 steps:10195\u001b[00m\n",
      "\u001b[94m #1735: episode_reward: 6.0 steps:10201\u001b[00m\n",
      "\u001b[94m #1736: episode_reward: 5.0 steps:10206\u001b[00m\n",
      "\u001b[94m #1737: episode_reward: 6.0 steps:10212\u001b[00m\n",
      "\u001b[94m #1738: episode_reward: 6.0 steps:10218\u001b[00m\n",
      "\u001b[94m #1739: episode_reward: 6.0 steps:10224\u001b[00m\n",
      "\u001b[94m #1740: episode_reward: 6.0 steps:10230\u001b[00m\n",
      "\u001b[94m #1741: episode_reward: 6.0 steps:10236\u001b[00m\n",
      "\u001b[94m #1742: episode_reward: 6.0 steps:10242\u001b[00m\n",
      "\u001b[94m #1743: episode_reward: 6.0 steps:10248\u001b[00m\n",
      "\u001b[94m #1744: episode_reward: 6.0 steps:10254\u001b[00m\n",
      "\u001b[94m #1745: episode_reward: 6.0 steps:10260\u001b[00m\n",
      "\u001b[94m #1746: episode_reward: 5.0 steps:10265\u001b[00m\n",
      "\u001b[94m #1747: episode_reward: 6.0 steps:10271\u001b[00m\n",
      "\u001b[94m #1748: episode_reward: 6.0 steps:10277\u001b[00m\n",
      "\u001b[94m #1749: episode_reward: 5.0 steps:10282\u001b[00m\n",
      "\u001b[94m #1750: episode_reward: 6.0 steps:10288\u001b[00m\n",
      "\u001b[94m #1751: episode_reward: 5.0 steps:10293\u001b[00m\n",
      "\u001b[94m #1752: episode_reward: 6.0 steps:10299\u001b[00m\n",
      "\u001b[94m #1753: episode_reward: 5.0 steps:10304\u001b[00m\n",
      "\u001b[94m #1754: episode_reward: 6.0 steps:10310\u001b[00m\n",
      "\u001b[94m #1755: episode_reward: 5.0 steps:10315\u001b[00m\n",
      "\u001b[94m #1756: episode_reward: 6.0 steps:10321\u001b[00m\n",
      "\u001b[94m #1757: episode_reward: 6.0 steps:10327\u001b[00m\n",
      "\u001b[94m #1758: episode_reward: 6.0 steps:10333\u001b[00m\n",
      "\u001b[94m #1759: episode_reward: 6.0 steps:10339\u001b[00m\n",
      "\u001b[94m #1760: episode_reward: 5.0 steps:10344\u001b[00m\n",
      "\u001b[94m #1761: episode_reward: 6.0 steps:10350\u001b[00m\n",
      "\u001b[94m #1762: episode_reward: 6.0 steps:10356\u001b[00m\n",
      "\u001b[94m #1763: episode_reward: 6.0 steps:10362\u001b[00m\n",
      "\u001b[94m #1764: episode_reward: 6.0 steps:10368\u001b[00m\n",
      "\u001b[94m #1765: episode_reward: 6.0 steps:10374\u001b[00m\n",
      "\u001b[94m #1766: episode_reward: 7.0 steps:10381\u001b[00m\n",
      "\u001b[94m #1767: episode_reward: 6.0 steps:10387\u001b[00m\n",
      "\u001b[94m #1768: episode_reward: 6.0 steps:10393\u001b[00m\n",
      "\u001b[94m #1769: episode_reward: 6.0 steps:10399\u001b[00m\n",
      "\u001b[94m #1770: episode_reward: 6.0 steps:10405\u001b[00m\n",
      "\u001b[94m #1771: episode_reward: 6.0 steps:10411\u001b[00m\n",
      "\u001b[94m #1772: episode_reward: 6.0 steps:10417\u001b[00m\n",
      "\u001b[94m #1773: episode_reward: 6.0 steps:10423\u001b[00m\n",
      "\u001b[94m #1774: episode_reward: 6.0 steps:10429\u001b[00m\n",
      "\u001b[94m #1775: episode_reward: 6.0 steps:10435\u001b[00m\n",
      "\u001b[94m #1776: episode_reward: 6.0 steps:10441\u001b[00m\n",
      "\u001b[94m #1777: episode_reward: 6.0 steps:10447\u001b[00m\n",
      "\u001b[94m #1778: episode_reward: 6.0 steps:10453\u001b[00m\n",
      "\u001b[94m #1779: episode_reward: 6.0 steps:10459\u001b[00m\n",
      "\u001b[94m #1780: episode_reward: 6.0 steps:10465\u001b[00m\n",
      "\u001b[94m #1781: episode_reward: 5.0 steps:10470\u001b[00m\n",
      "\u001b[94m #1782: episode_reward: 5.0 steps:10475\u001b[00m\n",
      "\u001b[94m #1783: episode_reward: 6.0 steps:10481\u001b[00m\n",
      "\u001b[94m #1784: episode_reward: 6.0 steps:10487\u001b[00m\n",
      "\u001b[94m #1785: episode_reward: 6.0 steps:10493\u001b[00m\n",
      "\u001b[94m #1786: episode_reward: 6.0 steps:10499\u001b[00m\n",
      "\u001b[94m #1787: episode_reward: 6.0 steps:10505\u001b[00m\n",
      "\u001b[94m #1788: episode_reward: 6.0 steps:10511\u001b[00m\n",
      "\u001b[94m #1789: episode_reward: 5.0 steps:10516\u001b[00m\n",
      "\u001b[94m #1790: episode_reward: 6.0 steps:10522\u001b[00m\n",
      "\u001b[94m #1791: episode_reward: 6.0 steps:10528\u001b[00m\n",
      "\u001b[94m #1792: episode_reward: 6.0 steps:10534\u001b[00m\n",
      "\u001b[94m #1793: episode_reward: 6.0 steps:10540\u001b[00m\n",
      "\u001b[94m #1794: episode_reward: 5.0 steps:10545\u001b[00m\n",
      "\u001b[94m #1795: episode_reward: 6.0 steps:10551\u001b[00m\n",
      "\u001b[94m #1796: episode_reward: 6.0 steps:10557\u001b[00m\n",
      "\u001b[94m #1797: episode_reward: 6.0 steps:10563\u001b[00m\n",
      "\u001b[94m #1798: episode_reward: 6.0 steps:10569\u001b[00m\n",
      "\u001b[94m #1799: episode_reward: 6.0 steps:10575\u001b[00m\n",
      "\u001b[94m #1800: episode_reward: 6.0 steps:10581\u001b[00m\n",
      "\u001b[94m #1801: episode_reward: 6.0 steps:10587\u001b[00m\n",
      "\u001b[94m #1802: episode_reward: 5.0 steps:10592\u001b[00m\n",
      "\u001b[94m #1803: episode_reward: 6.0 steps:10598\u001b[00m\n",
      "\u001b[94m #1804: episode_reward: 6.0 steps:10604\u001b[00m\n",
      "\u001b[94m #1805: episode_reward: 6.0 steps:10610\u001b[00m\n",
      "\u001b[94m #1806: episode_reward: 6.0 steps:10616\u001b[00m\n",
      "\u001b[94m #1807: episode_reward: 6.0 steps:10622\u001b[00m\n",
      "\u001b[94m #1808: episode_reward: 5.0 steps:10627\u001b[00m\n",
      "\u001b[94m #1809: episode_reward: 6.0 steps:10633\u001b[00m\n",
      "\u001b[94m #1810: episode_reward: 6.0 steps:10639\u001b[00m\n",
      "\u001b[94m #1811: episode_reward: 6.0 steps:10645\u001b[00m\n",
      "\u001b[94m #1812: episode_reward: 6.0 steps:10651\u001b[00m\n",
      "\u001b[94m #1813: episode_reward: 6.0 steps:10657\u001b[00m\n",
      "\u001b[94m #1814: episode_reward: 7.0 steps:10664\u001b[00m\n",
      "\u001b[94m #1815: episode_reward: 6.0 steps:10670\u001b[00m\n",
      "\u001b[94m #1816: episode_reward: 6.0 steps:10676\u001b[00m\n",
      "\u001b[94m #1817: episode_reward: 6.0 steps:10682\u001b[00m\n",
      "\u001b[94m #1818: episode_reward: 6.0 steps:10688\u001b[00m\n",
      "\u001b[94m #1819: episode_reward: 6.0 steps:10694\u001b[00m\n",
      "\u001b[94m #1820: episode_reward: 6.0 steps:10700\u001b[00m\n",
      "\u001b[94m #1821: episode_reward: 5.0 steps:10705\u001b[00m\n",
      "\u001b[94m #1822: episode_reward: 6.0 steps:10711\u001b[00m\n",
      "\u001b[94m #1823: episode_reward: 6.0 steps:10717\u001b[00m\n",
      "\u001b[94m #1824: episode_reward: 6.0 steps:10723\u001b[00m\n",
      "\u001b[94m #1825: episode_reward: 5.0 steps:10728\u001b[00m\n",
      "\u001b[94m #1826: episode_reward: 6.0 steps:10734\u001b[00m\n",
      "\u001b[94m #1827: episode_reward: 6.0 steps:10740\u001b[00m\n",
      "\u001b[94m #1828: episode_reward: 6.0 steps:10746\u001b[00m\n",
      "\u001b[94m #1829: episode_reward: 6.0 steps:10752\u001b[00m\n",
      "\u001b[94m #1830: episode_reward: 6.0 steps:10758\u001b[00m\n",
      "\u001b[94m #1831: episode_reward: 5.0 steps:10763\u001b[00m\n",
      "\u001b[94m #1832: episode_reward: 6.0 steps:10769\u001b[00m\n",
      "\u001b[94m #1833: episode_reward: 6.0 steps:10775\u001b[00m\n",
      "\u001b[94m #1834: episode_reward: 6.0 steps:10781\u001b[00m\n",
      "\u001b[94m #1835: episode_reward: 6.0 steps:10787\u001b[00m\n",
      "\u001b[94m #1836: episode_reward: 5.0 steps:10792\u001b[00m\n",
      "\u001b[94m #1837: episode_reward: 6.0 steps:10798\u001b[00m\n",
      "\u001b[94m #1838: episode_reward: 6.0 steps:10804\u001b[00m\n",
      "\u001b[94m #1839: episode_reward: 5.0 steps:10809\u001b[00m\n",
      "\u001b[94m #1840: episode_reward: 6.0 steps:10815\u001b[00m\n",
      "\u001b[94m #1841: episode_reward: 6.0 steps:10821\u001b[00m\n",
      "\u001b[94m #1842: episode_reward: 6.0 steps:10827\u001b[00m\n",
      "\u001b[94m #1843: episode_reward: 6.0 steps:10833\u001b[00m\n",
      "\u001b[94m #1844: episode_reward: 6.0 steps:10839\u001b[00m\n",
      "\u001b[94m #1845: episode_reward: 6.0 steps:10845\u001b[00m\n",
      "\u001b[94m #1846: episode_reward: 5.0 steps:10850\u001b[00m\n",
      "\u001b[94m #1847: episode_reward: 5.0 steps:10855\u001b[00m\n",
      "\u001b[94m #1848: episode_reward: 7.0 steps:10862\u001b[00m\n",
      "\u001b[94m #1849: episode_reward: 6.0 steps:10868\u001b[00m\n",
      "\u001b[94m #1850: episode_reward: 6.0 steps:10874\u001b[00m\n",
      "\u001b[94m #1851: episode_reward: 6.0 steps:10880\u001b[00m\n",
      "\u001b[94m #1852: episode_reward: 6.0 steps:10886\u001b[00m\n",
      "\u001b[94m #1853: episode_reward: 6.0 steps:10892\u001b[00m\n",
      "\u001b[94m #1854: episode_reward: 6.0 steps:10898\u001b[00m\n",
      "\u001b[94m #1855: episode_reward: 6.0 steps:10904\u001b[00m\n",
      "\u001b[94m #1856: episode_reward: 6.0 steps:10910\u001b[00m\n",
      "\u001b[94m #1857: episode_reward: 5.0 steps:10915\u001b[00m\n",
      "\u001b[94m #1858: episode_reward: 6.0 steps:10921\u001b[00m\n",
      "\u001b[94m #1859: episode_reward: 6.0 steps:10927\u001b[00m\n",
      "\u001b[94m #1860: episode_reward: 6.0 steps:10933\u001b[00m\n",
      "\u001b[94m #1861: episode_reward: 5.0 steps:10938\u001b[00m\n",
      "\u001b[94m #1862: episode_reward: 6.0 steps:10944\u001b[00m\n",
      "\u001b[94m #1863: episode_reward: 5.0 steps:10949\u001b[00m\n",
      "\u001b[94m #1864: episode_reward: 6.0 steps:10955\u001b[00m\n",
      "\u001b[94m #1865: episode_reward: 6.0 steps:10961\u001b[00m\n",
      "\u001b[94m #1866: episode_reward: 6.0 steps:10967\u001b[00m\n",
      "\u001b[94m #1867: episode_reward: 6.0 steps:10973\u001b[00m\n",
      "\u001b[94m #1868: episode_reward: 6.0 steps:10979\u001b[00m\n",
      "\u001b[94m #1869: episode_reward: 6.0 steps:10985\u001b[00m\n",
      "\u001b[94m #1870: episode_reward: 6.0 steps:10991\u001b[00m\n",
      "\u001b[94m #1871: episode_reward: 6.0 steps:10997\u001b[00m\n",
      "\u001b[94m #1872: episode_reward: 5.0 steps:11002\u001b[00m\n",
      "\u001b[94m #1873: episode_reward: 6.0 steps:11008\u001b[00m\n",
      "\u001b[94m #1874: episode_reward: 6.0 steps:11014\u001b[00m\n",
      "\u001b[94m #1875: episode_reward: 6.0 steps:11020\u001b[00m\n",
      "\u001b[94m #1876: episode_reward: 6.0 steps:11026\u001b[00m\n",
      "\u001b[94m #1877: episode_reward: 6.0 steps:11032\u001b[00m\n",
      "\u001b[94m #1878: episode_reward: 7.0 steps:11039\u001b[00m\n",
      "\u001b[94m #1879: episode_reward: 6.0 steps:11045\u001b[00m\n",
      "\u001b[94m #1880: episode_reward: 6.0 steps:11051\u001b[00m\n",
      "\u001b[94m #1881: episode_reward: 6.0 steps:11057\u001b[00m\n",
      "\u001b[94m #1882: episode_reward: 6.0 steps:11063\u001b[00m\n",
      "\u001b[94m #1883: episode_reward: 6.0 steps:11069\u001b[00m\n",
      "\u001b[94m #1884: episode_reward: 6.0 steps:11075\u001b[00m\n",
      "\u001b[94m #1885: episode_reward: 6.0 steps:11081\u001b[00m\n",
      "\u001b[94m #1886: episode_reward: 6.0 steps:11087\u001b[00m\n",
      "\u001b[94m #1887: episode_reward: 6.0 steps:11093\u001b[00m\n",
      "\u001b[94m #1888: episode_reward: 6.0 steps:11099\u001b[00m\n",
      "\u001b[94m #1889: episode_reward: 6.0 steps:11105\u001b[00m\n",
      "\u001b[94m #1890: episode_reward: 6.0 steps:11111\u001b[00m\n",
      "\u001b[94m #1891: episode_reward: 6.0 steps:11117\u001b[00m\n",
      "\u001b[94m #1892: episode_reward: 6.0 steps:11123\u001b[00m\n",
      "\u001b[94m #1893: episode_reward: 6.0 steps:11129\u001b[00m\n",
      "\u001b[94m #1894: episode_reward: 6.0 steps:11135\u001b[00m\n",
      "\u001b[94m #1895: episode_reward: 6.0 steps:11141\u001b[00m\n",
      "\u001b[94m #1896: episode_reward: 6.0 steps:11147\u001b[00m\n",
      "\u001b[94m #1897: episode_reward: 6.0 steps:11153\u001b[00m\n",
      "\u001b[94m #1898: episode_reward: 6.0 steps:11159\u001b[00m\n",
      "\u001b[94m #1899: episode_reward: 6.0 steps:11165\u001b[00m\n",
      "\u001b[94m #1900: episode_reward: 6.0 steps:11171\u001b[00m\n",
      "\u001b[94m #1901: episode_reward: 6.0 steps:11177\u001b[00m\n",
      "\u001b[94m #1902: episode_reward: 6.0 steps:11183\u001b[00m\n",
      "\u001b[94m #1903: episode_reward: 6.0 steps:11189\u001b[00m\n",
      "\u001b[94m #1904: episode_reward: 6.0 steps:11195\u001b[00m\n",
      "\u001b[94m #1905: episode_reward: 6.0 steps:11201\u001b[00m\n",
      "\u001b[94m #1906: episode_reward: 6.0 steps:11207\u001b[00m\n",
      "\u001b[94m #1907: episode_reward: 6.0 steps:11213\u001b[00m\n",
      "\u001b[94m #1908: episode_reward: 6.0 steps:11219\u001b[00m\n",
      "\u001b[94m #1909: episode_reward: 6.0 steps:11225\u001b[00m\n",
      "\u001b[94m #1910: episode_reward: 6.0 steps:11231\u001b[00m\n",
      "\u001b[94m #1911: episode_reward: 6.0 steps:11237\u001b[00m\n",
      "\u001b[94m #1912: episode_reward: 6.0 steps:11243\u001b[00m\n",
      "\u001b[94m #1913: episode_reward: 6.0 steps:11249\u001b[00m\n",
      "\u001b[94m #1914: episode_reward: 6.0 steps:11255\u001b[00m\n",
      "\u001b[94m #1915: episode_reward: 6.0 steps:11261\u001b[00m\n",
      "\u001b[94m #1916: episode_reward: 6.0 steps:11267\u001b[00m\n",
      "\u001b[94m #1917: episode_reward: 6.0 steps:11273\u001b[00m\n",
      "\u001b[94m #1918: episode_reward: 6.0 steps:11279\u001b[00m\n",
      "\u001b[94m #1919: episode_reward: 5.0 steps:11284\u001b[00m\n",
      "\u001b[94m #1920: episode_reward: 6.0 steps:11290\u001b[00m\n",
      "\u001b[94m #1921: episode_reward: 6.0 steps:11296\u001b[00m\n",
      "\u001b[94m #1922: episode_reward: 6.0 steps:11302\u001b[00m\n",
      "\u001b[94m #1923: episode_reward: 5.0 steps:11307\u001b[00m\n",
      "\u001b[94m #1924: episode_reward: 7.0 steps:11314\u001b[00m\n",
      "\u001b[94m #1925: episode_reward: 6.0 steps:11320\u001b[00m\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c87679b4e5d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     train(args.train_iter, agent, env, evaluate, \n\u001b[0m\u001b[0;32m     26\u001b[0m         args.validate_steps, args.output, max_episode_length=args.max_episode_length, debug=args.debug)\n\u001b[0;32m     27\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-7205c9983d97>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(num_iterations, agent, env, evaluate, validate_steps, output, max_episode_length, debug)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\87312\\Documents\\GitHub\\Storm-auto-scheduler\\Wolptinger\\wolptinger.py\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, s_t, decay_epsilon)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mproto_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay_epsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecay_epsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# print(f\"Proto action: {proto_action}, proto action.shape: {proto_action.shape}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mknn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mknn\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\87312\\Documents\\GitHub\\Storm-auto-scheduler\\Wolptinger\\action_space.py\u001b[0m in \u001b[0;36msearch_point\u001b[1;34m(self, point, k)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mp_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# print(\"p_in: {}, p_in.shape: {}, p_in.dtype: {}\".format(p_in, p_in.shape, p_in.dtype))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0msearch_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flann\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mknns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msearch_res\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mp_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyflann\\index.py\u001b[0m in \u001b[0;36mnn_index\u001b[1;34m(self, qpts, num_neighbors, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__flann_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         flann.find_nearest_neighbors_index[self.__curindex_type](self.__curindex,\n\u001b[0m\u001b[0;32m    234\u001b[0m                     \u001b[0mqpts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnqpts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_neighbors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args.output = get_output_folder(args.output, args.env)\n",
    "if args.resume == 'default':\n",
    "    args.resume = 'output/{}-run0'.format(args.env)\n",
    "\n",
    "# env = NormalizedEnv(gym.make(args.env))\n",
    "# env = gym.make(args.env)\n",
    "env = ContinuousCartPoleEnv()\n",
    "args.low = env.action_space.low\n",
    "args.high = env.action_space.high\n",
    "\n",
    "if args.seed > 0:\n",
    "    np.random.seed(args.seed)\n",
    "    env.seed(args.seed)\n",
    "\n",
    "nb_states = env.observation_space.shape[0]\n",
    "nb_actions = env.action_space.shape[0]\n",
    "\n",
    "agent = Wolptinger(nb_states, nb_actions, args)\n",
    "evaluate = Evaluator(args.validate_episodes, \n",
    "    args.validate_steps, args.output, max_episode_length=args.max_episode_length)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if args.mode == 'train':\n",
    "    train(args.train_iter, agent, env, evaluate, \n",
    "        args.validate_steps, args.output, max_episode_length=args.max_episode_length, debug=args.debug)\n",
    "    end_time = time.time()\n",
    "\n",
    "elif args.mode == 'test':\n",
    "    test(args.validate_episodes, agent, env, evaluate, args.resume,\n",
    "        visualize=True, debug=args.debug)\n",
    "\n",
    "else:\n",
    "    raise RuntimeError('undefined mode {}'.format(args.mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}