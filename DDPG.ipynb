{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "84b686020625b0e4c68be0b0fcd7ab3251cfecf2e3c65a384757419db216fdad"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import argparse\r\n",
    "from copy import deepcopy\r\n",
    "import gym\r\n",
    "import torch\r\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import sys\r\n",
    "sys.path.append(os.path.join(os.getcwd(), 'Simulator'))\r\n",
    "sys.path.append(os.path.join(os.getcwd(), 'Wolptinger'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from Wolptinger.ddpg import DDPG\r\n",
    "from Wolptinger.utils import *\r\n",
    "from Wolptinger.evaluator import Evaluator\r\n",
    "\r\n",
    "from Simulator.WordCounting import WordCountingEnv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "%load_ext autoreload\r\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def train(num_iterations, agent, env, evaluate, validate_steps, output, max_episode_length=None, debug=False):\r\n",
    "    agent.is_training = True\r\n",
    "    step = episode = episode_steps = 0\r\n",
    "    episode_reward = 0.\r\n",
    "    observation = None\r\n",
    "\r\n",
    "    while step < num_iterations:\r\n",
    "        # reset if it is the start of episode\r\n",
    "        if observation is None:\r\n",
    "            # the original method use deepcopy here\r\n",
    "            observation = env.reset()\r\n",
    "            agent.reset(observation)\r\n",
    "        \r\n",
    "        # agent choose action\r\n",
    "        if step < args.warmup:\r\n",
    "            action = agent.random_action()\r\n",
    "        else:\r\n",
    "            action = agent.select_action(observation)\r\n",
    "            print(action)\r\n",
    "        \r\n",
    "        \r\n",
    "        next_state, reward, done, info = env.step(action)\r\n",
    "        if max_episode_length and episode_steps >= max_episode_length - 1:\r\n",
    "            done = True\r\n",
    "        \r\n",
    "        # agent observe and update policy\r\n",
    "        agent.observe(reward, next_state, done)\r\n",
    "        if step > args.warmup:\r\n",
    "            agent.update_policy()\r\n",
    "        \r\n",
    "        # if debug:\r\n",
    "        #     prCyan(f'{step}: reward is {reward}')\r\n",
    "        \r\n",
    "        # evaluate the model\r\n",
    "        # I personally don't think this will work, also, this will let the model memory a wrong\r\n",
    "        # action\r\n",
    "        if evaluate is not None and validate_steps > 0 and step % validate_steps == 0:\r\n",
    "            policy = lambda x: agent.select_action(x, decay_epsilon=False)\r\n",
    "            validate_reward = evaluate(env, policy, debug=False, visualize=False, save=False)\r\n",
    "            if debug: \r\n",
    "                prYellow('[Evaluate] Step_{:07d}: mean_reward:{}'.format(step, validate_reward))\r\n",
    "            agent.s_t = observation\r\n",
    "            agent.a_t = action\r\n",
    "\r\n",
    "        # save intermidate training model\r\n",
    "        if step % int(num_iterations/3) == 0:\r\n",
    "            agent.save_model(output)\r\n",
    "        \r\n",
    "        # update models\r\n",
    "        step += 1\r\n",
    "        episode_steps += 1\r\n",
    "        episode_reward += reward\r\n",
    "        observation = next_state\r\n",
    "        \r\n",
    "        # end of episode\r\n",
    "        if done:\r\n",
    "            if debug:\r\n",
    "                prLightPurple(f'#{episode}: episode_reward: {episode_reward} steps:{step}')\r\n",
    "            \r\n",
    "            # agent.memory.append(\r\n",
    "            #     observation,\r\n",
    "            #     agent.select_action(observation)\r\n",
    "            #     0.,\r\n",
    "            #     False\r\n",
    "            # )\r\n",
    "\r\n",
    "            # reset\r\n",
    "            observation = None\r\n",
    "            episode_steps = 0\r\n",
    "            episode_reward = 0.\r\n",
    "            episode += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def test(num_episodes, agent, env, evaluate, model_path, visualize=True, debug=False):\r\n",
    "\r\n",
    "    agent.load_weights(model_path)\r\n",
    "    agent.is_training = False\r\n",
    "    agent.eval()\r\n",
    "    policy = lambda x: agent.select_action(x, decay_epsilon=False)\r\n",
    "\r\n",
    "    for i in range(num_episodes):\r\n",
    "        validate_reward = evaluate(env, policy, debug=debug, visualize=visualize, save=False)\r\n",
    "        if debug: \r\n",
    "            prYellow('[Evaluate] #{}: mean_reward:{}'.format(i, validate_reward))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class Arguments(object):\r\n",
    "    def __init__(self):\r\n",
    "        self.mode = 'train'\r\n",
    "        # self.env = \"InvertedPendulum-v2\"\r\n",
    "        self.env = 'WordCountingEnv'\r\n",
    "        self.h1 = 128\r\n",
    "        self.h2 = 128\r\n",
    "        self.rate = 1e-3\r\n",
    "        self.prate = 1e-3\r\n",
    "        self.warmup = 100\r\n",
    "        self.discount = 0.99\r\n",
    "        self.bsize = 64\r\n",
    "        self.rmsize = 10000\r\n",
    "        self.window_length = 1\r\n",
    "        self.tau = 0.001\r\n",
    "        self.ou_theta = 0.15\r\n",
    "        self.ou_sigma = 0.2\r\n",
    "        self.ou_mu = 0.0\r\n",
    "        self.validate_episodes = 20\r\n",
    "        self.max_episode_length = 50\r\n",
    "        self.validate_steps = 2000\r\n",
    "        self.output = 'output'\r\n",
    "        self.debug = True\r\n",
    "        self.init_w = 0.003\r\n",
    "        self.train_iter = 15000\r\n",
    "        # self.epsilon = 50000\r\n",
    "        self.epsilon = 10000\r\n",
    "        self.seed = -1\r\n",
    "        self.resume = 'default'\r\n",
    "        self.k_ratio = 1e-6\r\n",
    "        # we pass in the pre_generated action space\r\n",
    "        self.action_space = None\r\n",
    "\r\n",
    "args = Arguments()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "args.output = get_output_folder(args.output, args.env)\r\n",
    "if args.resume == 'default':\r\n",
    "    args.resume = 'WordCountingDDPG/{}-run0'.format(args.env)\r\n",
    "\r\n",
    "# env = NormalizedEnv(gym.make(args.env))\r\n",
    "# env = gym.make(args.env)\r\n",
    "env = WordCountingEnv()\r\n",
    "args.low = env.action_space.low\r\n",
    "args.high = env.action_space.high\r\n",
    "\r\n",
    "if args.seed > 0:\r\n",
    "    np.random.seed(args.seed)\r\n",
    "    env.seed(args.seed)\r\n",
    "\r\n",
    "nb_states = env.observation_space.shape[0]\r\n",
    "nb_actions = env.action_space.shape[0]\r\n",
    "\r\n",
    "agent = DDPG(nb_states, nb_actions, args)\r\n",
    "evaluate = Evaluator(args.validate_episodes, \r\n",
    "    args.validate_steps, args.output, max_episode_length=args.max_episode_length)\r\n",
    "\r\n",
    "start_time = time.time()\r\n",
    "\r\n",
    "if args.mode == 'train':\r\n",
    "    train(args.train_iter, agent, env, None, \r\n",
    "        args.validate_steps, args.output, max_episode_length=args.max_episode_length, debug=args.debug)\r\n",
    "    end_time = time.time()\r\n",
    "\r\n",
    "elif args.mode == 'test':\r\n",
    "    test(args.validate_episodes, agent, env, evaluate, args.resume,\r\n",
    "        visualize=True, debug=args.debug)\r\n",
    "\r\n",
    "else:\r\n",
    "    raise RuntimeError('undefined mode {}'.format(args.mode))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building Topology\n",
      "spout 2\n",
      "WordCount 9\n",
      "Database 9\n",
      "torch.Size([128, 17])\n",
      "torch.Size([128, 17])\n",
      "\u001b[96m 0: reward is -1.7222163899998169\u001b[00m\n",
      "\u001b[96m 1: reward is -1.8696246599998745\u001b[00m\n",
      "\u001b[96m 2: reward is -1.752544070000428\u001b[00m\n",
      "\u001b[96m 3: reward is -1.7792574400003514\u001b[00m\n",
      "\u001b[96m 4: reward is -1.5527319100002153\u001b[00m\n",
      "\u001b[96m 5: reward is -1.7818512800003588\u001b[00m\n",
      "\u001b[96m 6: reward is -1.8534155400002992\u001b[00m\n",
      "\u001b[96m 7: reward is -1.7785978500006017\u001b[00m\n",
      "\u001b[96m 8: reward is -1.669319539999811\u001b[00m\n",
      "\u001b[96m 9: reward is -1.6885948700001234\u001b[00m\n",
      "\u001b[96m 10: reward is -1.6419122299996682\u001b[00m\n",
      "\u001b[96m 11: reward is -1.6859567200004002\u001b[00m\n",
      "\u001b[96m 12: reward is -1.5615080999997648\u001b[00m\n",
      "\u001b[96m 13: reward is -1.6870391199994796\u001b[00m\n",
      "\u001b[96m 14: reward is -1.5944494599996875\u001b[00m\n",
      "\u001b[96m 15: reward is -1.8363346100011435\u001b[00m\n",
      "\u001b[96m 16: reward is -1.8443893199983803\u001b[00m\n",
      "\u001b[96m 17: reward is -1.6805662600000584\u001b[00m\n",
      "\u001b[96m 18: reward is -1.6444585100004785\u001b[00m\n",
      "\u001b[96m 19: reward is -1.5479083900008546\u001b[00m\n",
      "\u001b[96m 20: reward is -1.7484906400002935\u001b[00m\n",
      "\u001b[96m 21: reward is -1.8481413300007308\u001b[00m\n",
      "\u001b[96m 22: reward is -1.804410749999896\u001b[00m\n",
      "\u001b[96m 23: reward is -1.5375309000005792\u001b[00m\n",
      "\u001b[96m 24: reward is -1.7558834700000658\u001b[00m\n",
      "\u001b[96m 25: reward is -1.9034124600003317\u001b[00m\n",
      "\u001b[96m 26: reward is -1.8573657099994936\u001b[00m\n",
      "\u001b[96m 27: reward is -1.7074272599985736\u001b[00m\n",
      "\u001b[96m 28: reward is -1.7293906599994562\u001b[00m\n",
      "\u001b[96m 29: reward is -1.6133972300015582\u001b[00m\n",
      "\u001b[96m 30: reward is -1.7383559999968792\u001b[00m\n",
      "\u001b[96m 31: reward is -1.6840752400011039\u001b[00m\n",
      "\u001b[96m 32: reward is -1.5961040799976973\u001b[00m\n",
      "\u001b[96m 33: reward is -1.6244437200021686\u001b[00m\n",
      "\u001b[96m 34: reward is -1.5935647000001272\u001b[00m\n",
      "\u001b[96m 35: reward is -1.652375469999878\u001b[00m\n",
      "\u001b[96m 36: reward is -1.5672564899974322\u001b[00m\n",
      "\u001b[96m 37: reward is -1.7080898899988166\u001b[00m\n",
      "\u001b[96m 38: reward is -1.5876966900002014\u001b[00m\n",
      "\u001b[96m 39: reward is -1.8297245300007616\u001b[00m\n",
      "\u001b[96m 40: reward is -1.7682341400001094\u001b[00m\n",
      "\u001b[96m 41: reward is -1.6363608899983164\u001b[00m\n",
      "\u001b[96m 42: reward is -1.7466564299973752\u001b[00m\n",
      "\u001b[96m 43: reward is -1.5702149000012362\u001b[00m\n",
      "\u001b[96m 44: reward is -1.7004000700007151\u001b[00m\n",
      "\u001b[96m 45: reward is -1.729664570002777\u001b[00m\n",
      "\u001b[96m 46: reward is -1.7536463299991067\u001b[00m\n",
      "\u001b[96m 47: reward is -1.614415379998656\u001b[00m\n",
      "\u001b[96m 48: reward is -1.8828334199973866\u001b[00m\n",
      "\u001b[96m 49: reward is -1.6083689099995675\u001b[00m\n",
      "\u001b[94m #0: episode_reward: -85.27060852999305 steps:50\u001b[00m\n",
      "\u001b[96m 50: reward is -1.6795718700016844\u001b[00m\n",
      "\u001b[96m 51: reward is -1.7205155799979204\u001b[00m\n",
      "\u001b[96m 52: reward is -1.5755125499994296\u001b[00m\n",
      "\u001b[96m 53: reward is -1.6410645999977242\u001b[00m\n",
      "\u001b[96m 54: reward is -1.6628836899968271\u001b[00m\n",
      "\u001b[96m 55: reward is -1.786415140000257\u001b[00m\n",
      "\u001b[96m 56: reward is -1.7705068600026215\u001b[00m\n",
      "\u001b[96m 57: reward is -1.704758169999246\u001b[00m\n",
      "\u001b[96m 58: reward is -1.6699281599989546\u001b[00m\n",
      "\u001b[96m 59: reward is -1.5042350600028689\u001b[00m\n",
      "\u001b[96m 60: reward is -1.623805790004842\u001b[00m\n",
      "\u001b[96m 61: reward is -1.6000988900017215\u001b[00m\n",
      "\u001b[96m 62: reward is -1.5401366300040122\u001b[00m\n",
      "\u001b[96m 63: reward is -1.510604170001934\u001b[00m\n",
      "\u001b[96m 64: reward is -1.5081948100013238\u001b[00m\n",
      "\u001b[96m 65: reward is -1.7446164499981376\u001b[00m\n",
      "\u001b[96m 66: reward is -1.5819927000011307\u001b[00m\n",
      "\u001b[96m 67: reward is -1.7838588799991377\u001b[00m\n",
      "\u001b[96m 68: reward is -1.5695576999974508\u001b[00m\n",
      "\u001b[96m 69: reward is -1.734432910001009\u001b[00m\n",
      "\u001b[96m 70: reward is -1.5377386700036912\u001b[00m\n",
      "\u001b[96m 71: reward is -1.6198960499976098\u001b[00m\n",
      "\u001b[96m 72: reward is -1.8370424699978856\u001b[00m\n",
      "\u001b[96m 73: reward is -1.6847474000012468\u001b[00m\n",
      "\u001b[96m 74: reward is -1.689088810001876\u001b[00m\n",
      "\u001b[96m 75: reward is -1.7186191500015615\u001b[00m\n",
      "\u001b[96m 76: reward is -1.745904879999765\u001b[00m\n",
      "\u001b[96m 77: reward is -1.7389527600030286\u001b[00m\n",
      "\u001b[96m 78: reward is -1.5575411900015688\u001b[00m\n",
      "\u001b[96m 79: reward is -1.709470270006666\u001b[00m\n",
      "\u001b[96m 80: reward is -1.7275270600039279\u001b[00m\n",
      "\u001b[96m 81: reward is -1.4356864199983934\u001b[00m\n",
      "\u001b[96m 82: reward is -1.6765764399977576\u001b[00m\n",
      "\u001b[96m 83: reward is -1.7471016799999448\u001b[00m\n",
      "\u001b[96m 84: reward is -1.6267497099982946\u001b[00m\n",
      "\u001b[96m 85: reward is -1.7661431600040163\u001b[00m\n",
      "\u001b[96m 86: reward is -1.5045239400008577\u001b[00m\n",
      "\u001b[96m 87: reward is -1.8172386400019191\u001b[00m\n",
      "\u001b[96m 88: reward is -1.7845175600002392\u001b[00m\n",
      "\u001b[96m 89: reward is -1.7086637800009992\u001b[00m\n",
      "\u001b[96m 90: reward is -1.6406069400077663\u001b[00m\n",
      "\u001b[96m 91: reward is -1.684869980002232\u001b[00m\n",
      "\u001b[96m 92: reward is -1.937165769996586\u001b[00m\n",
      "\u001b[96m 93: reward is -1.5725235400044395\u001b[00m\n",
      "\u001b[96m 94: reward is -1.851004509995567\u001b[00m\n",
      "\u001b[96m 95: reward is -1.6250860800020595\u001b[00m\n",
      "\u001b[96m 96: reward is -1.5185589399992057\u001b[00m\n",
      "\u001b[96m 97: reward is -1.5036158399975184\u001b[00m\n",
      "\u001b[96m 98: reward is -1.5082849399974454\u001b[00m\n",
      "\u001b[96m 99: reward is -1.9623067400054273\u001b[00m\n",
      "\u001b[94m #1: episode_reward: -83.35044393003774 steps:100\u001b[00m\n",
      "[-0.05729027 -0.01685535  0.09531862 -0.05315527  0.03985752 -0.02658412\n",
      " -0.07900629  0.03981515  0.03617407  0.01999045 -0.11571557 -0.04316732\n",
      " -0.04660597  0.08282266  0.0004895 ]\n",
      "\u001b[96m 100: reward is -1.546761550001969\u001b[00m\n",
      "[-0.03844654  0.00226081  0.1215819  -0.0735141   0.06346475 -0.01213695\n",
      " -0.08218995  0.02712224  0.04293695  0.01788442 -0.10478773 -0.05252161\n",
      " -0.03681129  0.08435908 -0.02488217]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "c:\\Users\\87312\\Documents\\GitHub\\Storm-auto-scheduler\\Wolptinger\\utils.py:23: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(\n",
      "c:\\Users\\87312\\Documents\\GitHub\\Storm-auto-scheduler\\Wolptinger\\ddpg.py:140: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  next_q_values.volatile=False\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[96m 101: reward is -1.5659244299993245\u001b[00m\n",
      "[ 0.06516823  0.11785434  0.02791397 -0.14884204  0.1198983   0.05136436\n",
      "  0.04915169  0.13614656 -0.03353372 -0.08920311  0.01615299  0.03847342\n",
      " -0.1017075  -0.0086747  -0.1237223 ]\n",
      "\u001b[96m 102: reward is -1.5968825999996346\u001b[00m\n",
      "[ 0.18322055  0.21276349 -0.06742755 -0.23770496  0.18557934  0.15207975\n",
      "  0.12226377  0.20514634 -0.13555361 -0.18081748  0.11856009  0.09364305\n",
      " -0.08795589 -0.10496526 -0.23731962]\n",
      "\u001b[96m 103: reward is -1.5915890000011226\u001b[00m\n",
      "[ 0.29013205  0.33842716 -0.22619314 -0.35250434  0.28674763  0.27323574\n",
      "  0.18270303  0.30739683 -0.22035141 -0.337819    0.24536519  0.04608582\n",
      " -0.02764674 -0.23514836 -0.36312115]\n",
      "\u001b[96m 104: reward is -1.6619202200024912\u001b[00m\n",
      "[ 0.42546284  0.47937873 -0.4260175  -0.47522664  0.31862664  0.33527112\n",
      "  0.14246957  0.3830762  -0.36543766 -0.43298492  0.38440472 -0.05051708\n",
      "  0.07771321 -0.3945063  -0.5080956 ]\n",
      "\u001b[96m 105: reward is -1.5872966699998268\u001b[00m\n",
      "[ 0.5785282   0.6146932  -0.57545906 -0.6014276   0.29014695  0.3645489\n",
      "  0.0745764   0.47250226 -0.5014543  -0.56897175  0.5311476  -0.16807742\n",
      "  0.22878885 -0.52306926 -0.6774628 ]\n",
      "\u001b[96m 106: reward is -1.570342360002892\u001b[00m\n",
      "[ 0.6934081   0.7548693  -0.68676823 -0.72468823  0.21825668  0.318595\n",
      "  0.01799792  0.57841325 -0.5821519  -0.7188817   0.70213944 -0.30552572\n",
      "  0.36252558 -0.6555818  -0.8248609 ]\n",
      "\u001b[96m 107: reward is -1.6742754800014221\u001b[00m\n",
      "[ 0.8503139   0.8891087  -0.7867303  -0.79193145  0.14667237  0.24475002\n",
      " -0.09465171  0.70719975 -0.5871149  -0.82321525  0.78619456 -0.45006067\n",
      "  0.543022   -0.76903975 -0.96213466]\n",
      "\u001b[96m 108: reward is -1.7253247400038154\u001b[00m\n",
      "[ 0.9624669   0.9588745  -0.9078235  -0.8653597   0.00441914  0.16113284\n",
      " -0.23062183  0.8160248  -0.5823755  -0.92816466  0.88500047 -0.6057492\n",
      "  0.6615085  -0.85814893 -1.        ]\n",
      "\u001b[96m 109: reward is -1.753763889997336\u001b[00m\n",
      "[ 1.          1.         -0.99247485 -0.8976214  -0.163721    0.03634065\n",
      " -0.42028898  0.8636339  -0.547254   -0.973935    0.9309369  -0.7299888\n",
      "  0.772174   -0.8993198  -1.        ]\n",
      "\u001b[96m 110: reward is -1.755356600001885\u001b[00m\n",
      "[ 1.          1.         -1.         -0.93292886 -0.3732807  -0.15608002\n",
      " -0.6134231   0.9338254  -0.48805383 -1.          0.94626874 -0.8176869\n",
      "  0.8997642  -0.9317013  -1.        ]\n",
      "\u001b[96m 111: reward is -1.8553426400072976\u001b[00m\n",
      "[ 1.          1.         -1.         -0.9228918  -0.5989019  -0.3329342\n",
      " -0.76343864  0.96665233 -0.36296827 -1.          0.9375478  -0.8767503\n",
      "  0.9500503  -0.97865325 -1.        ]\n",
      "\u001b[96m 112: reward is -1.9007324000039139\u001b[00m\n",
      "[ 1.          1.         -1.         -0.91912735 -0.7588828  -0.54574174\n",
      " -0.8770348   0.9816891  -0.20099084 -1.          0.94104457 -0.9221366\n",
      "  0.9977618  -0.9926397  -1.        ]\n",
      "\u001b[96m 113: reward is -1.9299682399956044\u001b[00m\n",
      "[ 1.          1.         -1.         -0.9181578  -0.8891083  -0.6859945\n",
      " -0.9831177   0.9671878  -0.03838459 -1.          0.9488176  -0.94675976\n",
      "  1.         -0.9849033  -1.        ]\n",
      "\u001b[96m 114: reward is -1.8941365200104845\u001b[00m\n",
      "[ 1.          1.         -1.         -0.95327693 -0.9292126  -0.7895404\n",
      " -1.          0.93248266  0.18975586 -1.          0.9242756  -0.9305608\n",
      "  1.         -1.         -1.        ]\n",
      "\u001b[96m 115: reward is -1.96205711001087\u001b[00m\n",
      "[ 1.          1.         -1.         -0.9525078  -0.9869175  -0.8234728\n",
      " -1.          0.911735    0.47328532 -1.          0.9331374  -0.95794296\n",
      "  1.         -0.97481227 -1.        ]\n",
      "\u001b[96m 116: reward is -1.9609833300115798\u001b[00m\n",
      "[ 1.          1.         -1.         -0.9440143  -1.         -0.84791416\n",
      " -1.          0.9587845   0.6975472  -1.          0.95655555 -0.97631264\n",
      "  1.         -0.9795372  -1.        ]\n",
      "\u001b[96m 117: reward is -1.9246377899962477\u001b[00m\n",
      "[ 1.          1.         -1.         -0.91260815 -1.         -0.88502425\n",
      " -1.          0.9476246   0.8741914  -1.          0.9570736  -0.94282025\n",
      "  1.         -1.         -1.        ]\n",
      "\u001b[96m 118: reward is -1.9439063600024884\u001b[00m\n",
      "[ 1.          1.         -1.         -0.91053903 -1.         -0.89807445\n",
      " -1.          0.9382422   0.9848093  -1.          0.96407425 -0.9574298\n",
      "  1.         -1.         -1.        ]\n",
      "\u001b[96m 119: reward is -1.9542758499999926\u001b[00m\n",
      "[ 1.          1.         -1.         -0.8978874  -1.         -0.9246878\n",
      " -1.          0.89373535  1.         -0.9937421   0.9630809  -0.9600582\n",
      "  1.         -0.99407035 -1.        ]\n",
      "\u001b[96m 120: reward is -1.934557080012611\u001b[00m\n",
      "[ 1.         1.        -1.        -0.9293006 -1.        -0.9076611\n",
      " -1.         0.8756842  1.        -0.9973928  0.9706533 -0.9692094\n",
      "  1.        -0.9529265 -1.       ]\n",
      "\u001b[96m 121: reward is -1.9427136699995562\u001b[00m\n",
      "[ 1.          1.         -1.         -0.91106653 -1.         -0.8908002\n",
      " -1.          0.90233004  1.         -0.99323857  0.98502195 -0.95353025\n",
      "  1.         -0.94878834 -1.        ]\n",
      "\u001b[96m 122: reward is -1.957504330000321\u001b[00m\n",
      "[ 1.          1.         -1.         -0.91681975 -1.         -0.88422984\n",
      " -1.          0.8820924   1.         -1.          0.9902121  -0.9131148\n",
      "  1.         -0.85882473 -1.        ]\n",
      "\u001b[96m 123: reward is -1.9472782500114012\u001b[00m\n",
      "[ 0.98047477  1.         -1.         -0.8837227  -1.         -0.881356\n",
      " -1.          0.9169554   1.         -1.          0.9957489  -0.93051386\n",
      "  1.         -0.68998456 -1.        ]\n",
      "\u001b[96m 124: reward is -1.9706854500105022\u001b[00m\n",
      "[ 0.9608393   1.         -1.         -0.9161033  -1.         -0.90078264\n",
      " -1.          0.93363255  1.         -1.          1.         -0.9384943\n",
      "  1.         -0.49101475 -1.        ]\n",
      "\u001b[96m 125: reward is -1.9619576699956507\u001b[00m\n",
      "[ 0.9401859   1.         -1.         -0.8870433  -1.         -0.90735227\n",
      " -1.          0.9450287   1.         -1.          1.         -0.9267382\n",
      "  1.         -0.16615036 -1.        ]\n",
      "\u001b[96m 126: reward is -1.862514839998025\u001b[00m\n",
      "[ 0.9438057   1.         -1.         -0.8737651  -1.         -0.8979907\n",
      " -1.          0.95198095  1.         -1.          1.         -0.9654713\n",
      "  1.          0.23322907 -1.        ]\n",
      "\u001b[96m 127: reward is -1.8886025900001615\u001b[00m\n",
      "[ 0.9183234   1.         -1.         -0.88798463 -1.         -0.9243671\n",
      " -1.          0.9540915   1.         -1.          1.         -0.9508604\n",
      "  1.          0.6003397  -1.        ]\n",
      "\u001b[96m 128: reward is -1.9040699699967774\u001b[00m\n",
      "[ 0.9229226   1.         -1.         -0.9101255  -1.         -0.93986815\n",
      " -1.          0.94534993  1.         -1.          1.         -0.9390169\n",
      "  1.          0.847831   -1.        ]\n",
      "\u001b[96m 129: reward is -1.8967025800009025\u001b[00m\n",
      "[ 0.9055013   1.         -1.         -0.9294137  -1.         -0.9332336\n",
      " -1.          0.9244033   1.         -0.9733098   1.         -0.96093255\n",
      "  1.          0.9582835  -1.        ]\n",
      "\u001b[96m 130: reward is -1.861461040008224\u001b[00m\n",
      "[ 0.8948135   1.         -1.         -0.95941734 -1.         -0.90596056\n",
      " -1.          0.90718395  1.         -0.9783286   1.         -0.94719416\n",
      "  1.          0.9969361  -1.        ]\n",
      "\u001b[96m 131: reward is -1.864081980000639\u001b[00m\n",
      "[ 0.91241753  1.         -1.         -0.9628802  -1.         -0.8995508\n",
      " -1.          0.87401736  1.         -0.99577934  1.         -0.96205866\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 132: reward is -1.8644925799976826\u001b[00m\n",
      "[ 0.9206449  1.        -1.        -0.96967   -1.        -0.8928437\n",
      " -1.         0.8995134  1.        -0.9901619  1.        -0.9782702\n",
      "  1.         1.        -1.       ]\n",
      "\u001b[96m 133: reward is -1.8606752599909087\u001b[00m\n",
      "[ 0.92868465  1.         -1.         -0.97484124 -1.         -0.8746842\n",
      " -1.          0.8798977   1.         -1.          1.         -0.984974\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 134: reward is -1.8665343499889224\u001b[00m\n",
      "[ 0.93425566  1.         -1.         -0.9446239  -1.         -0.9098243\n",
      " -1.          0.9200173   1.         -1.          1.         -1.\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 135: reward is -1.863503719998342\u001b[00m\n",
      "[ 0.92381763  1.         -1.         -0.93461704 -1.         -0.898742\n",
      " -1.          0.9534006   1.         -1.          1.         -1.\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 136: reward is -1.8951638199929217\u001b[00m\n",
      "[ 0.8941514   1.         -1.         -0.93063337 -1.         -0.8544465\n",
      " -1.          0.9404493   1.         -1.          1.         -0.994955\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 137: reward is -1.8654769700039084\u001b[00m\n",
      "[ 0.88689935  1.         -1.         -0.923569   -1.         -0.8741336\n",
      " -1.          0.9671995   1.         -1.          1.         -1.\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 138: reward is -1.8675066799940867\u001b[00m\n",
      "[ 0.8812852   1.         -1.         -0.9296781  -1.         -0.8755028\n",
      " -1.          0.9351683   1.         -1.          0.98831415 -1.\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 139: reward is -1.871765259991039\u001b[00m\n",
      "[ 0.8558035   1.         -1.         -0.9311831  -1.         -0.8268987\n",
      " -1.          0.92804694  1.         -1.          0.9823563  -1.\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 140: reward is -1.8715959199991288\u001b[00m\n",
      "[ 0.8756539   1.         -1.         -0.9240489  -1.         -0.79742986\n",
      " -1.          0.9305343   1.         -1.          0.9975287  -1.\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 141: reward is -1.8332740900031896\u001b[00m\n",
      "[ 0.8456362   1.         -1.         -0.90596396 -1.         -0.81398565\n",
      " -1.          0.9267929   1.         -1.          0.9624043  -1.\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 142: reward is -1.8521678100099368\u001b[00m\n",
      "[ 0.8786213   1.         -1.         -0.955489   -1.         -0.77678686\n",
      " -1.          0.90978634  1.         -1.          0.9512051  -1.\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 143: reward is -1.8374935000045016\u001b[00m\n",
      "[ 0.89879346  1.         -1.         -0.9633134  -1.         -0.7392888\n",
      " -1.          0.8690994   1.         -1.          0.9467979  -1.\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 144: reward is -1.8593392999986256\u001b[00m\n",
      "[ 0.8989784   1.         -1.         -0.96883804 -1.         -0.700193\n",
      " -1.          0.87439895  1.         -1.          0.947903   -1.\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 145: reward is -1.8616273299981676\u001b[00m\n",
      "[ 0.890644    1.         -1.         -0.9587449  -1.         -0.59569615\n",
      " -1.          0.867397    1.         -0.9824129   0.9529255  -1.\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 146: reward is -1.836201579993409\u001b[00m\n",
      "[ 0.86441547  1.         -1.         -0.9445081  -1.         -0.43069252\n",
      " -1.          0.88481116  0.9982185  -0.9937943   0.9558563  -1.\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 147: reward is -1.8422790599947143\u001b[00m\n",
      "[ 0.8794885   1.         -1.         -0.97445387 -1.         -0.07109184\n",
      " -1.          0.885588    0.9886166  -1.          0.9450116  -1.\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 148: reward is -1.885631859999809\u001b[00m\n",
      "[ 0.87098795  1.         -1.         -0.99767    -1.          0.46470362\n",
      " -1.          0.87351644  0.9826616  -1.          0.9254021  -0.99552995\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 149: reward is -1.7838259299976704\u001b[00m\n",
      "\u001b[94m #2: episode_reward: -91.76616225004192 steps:150\u001b[00m\n",
      "[ 1.          1.         -0.9976886  -1.         -0.98994046  0.68742794\n",
      " -1.          1.          0.9944384  -1.          1.         -1.\n",
      "  0.99953806  0.9893984  -0.9823031 ]\n",
      "\u001b[96m 150: reward is -1.7767571299960139\u001b[00m\n",
      "[ 1.         1.        -1.        -0.9732897 -1.         0.934883\n",
      " -1.         1.         1.        -0.9910226  1.        -1.\n",
      "  0.9945572  0.9665944 -0.9495852]\n",
      "\u001b[96m 151: reward is -1.6857979300088948\u001b[00m\n",
      "[ 1.          1.         -0.99940026 -0.92962503 -1.          1.\n",
      " -1.          1.          1.         -0.99600565  1.         -1.\n",
      "  0.99553204  0.9660103  -0.9391722 ]\n",
      "\u001b[96m 152: reward is -1.7047948500042105\u001b[00m\n",
      "[ 0.9933725   1.         -0.9999609  -0.9837186  -1.          1.\n",
      " -0.97426903  1.          1.         -1.          1.         -1.\n",
      "  0.9995234   0.99158293 -0.9218404 ]\n",
      "\u001b[96m 153: reward is -1.7169855800012988\u001b[00m\n",
      "[ 0.9994559   1.         -0.98516923 -0.9896219  -0.9897995   1.\n",
      " -0.95273584  1.          1.         -1.          1.         -1.\n",
      "  0.9951909   0.98877746 -0.929711  ]\n",
      "\u001b[96m 154: reward is -1.6940590899892665\u001b[00m\n",
      "[ 1.          0.9998221  -0.95763236 -1.         -0.9977667   1.\n",
      " -0.9212511   1.          1.         -1.          1.         -0.9984742\n",
      "  0.99603695  0.998242   -0.9026176 ]\n",
      "\u001b[96m 155: reward is -1.7075724900003593\u001b[00m\n",
      "[ 0.9866563   0.9983662  -0.9428454  -0.98804677 -1.          1.\n",
      " -0.9198678   1.          1.         -1.          1.         -0.9876701\n",
      "  0.98982364  0.9806482  -0.8768744 ]\n",
      "\u001b[96m 156: reward is -1.701988350013548\u001b[00m\n",
      "[ 1.          0.9784559  -0.9525749  -1.         -1.          1.\n",
      " -0.93808347  1.          0.99777377 -1.          1.         -0.988748\n",
      "  0.98092014  1.         -0.89912   ]\n",
      "\u001b[96m 157: reward is -1.68686303000062\u001b[00m\n",
      "[ 1.          0.97847414 -0.93190515 -0.9893735  -0.9803371   1.\n",
      " -0.94483596  1.          0.93479174 -1.          1.         -1.\n",
      "  1.          1.         -0.93257326]\n",
      "\u001b[96m 158: reward is -1.6994839099939634\u001b[00m\n",
      "[ 1.          0.94886315 -0.92526716 -1.         -0.9872986   1.\n",
      " -0.9782784   1.          0.9577702  -1.          1.         -1.\n",
      "  0.9967191   1.         -0.96310896]\n",
      "\u001b[96m 159: reward is -1.705607099993259\u001b[00m\n",
      "[ 1.          0.9786282  -0.88196874 -1.         -0.9483483   1.\n",
      " -0.9800695   1.          0.93328434 -1.          1.         -1.\n",
      "  0.984663    1.         -0.9575551 ]\n",
      "\u001b[96m 160: reward is -1.706016429994416\u001b[00m\n",
      "[ 1.          0.99187356 -0.8917084  -1.         -0.92084736  1.\n",
      " -0.9758222   1.          0.9221953  -1.          1.         -1.\n",
      "  0.9926911   1.         -0.95692474]\n",
      "\u001b[96m 161: reward is -1.6979463799972738\u001b[00m\n",
      "[ 1.          1.         -0.9190752  -1.         -0.9108789   1.\n",
      " -0.9516159   1.          0.88834494 -1.          1.         -1.\n",
      "  1.          1.         -0.9407493 ]\n",
      "\u001b[96m 162: reward is -1.7158579399909497\u001b[00m\n",
      "[ 1.          0.96287656 -0.9495959  -1.         -0.9352338   1.\n",
      " -0.95321685  1.          0.88572526 -1.          1.         -1.\n",
      "  1.          1.         -0.927245  ]\n",
      "\u001b[96m 163: reward is -1.7008960000003712\u001b[00m\n",
      "[ 1.          0.9985203  -0.92681766 -1.         -0.93770415  1.\n",
      " -0.93242     1.          0.83819205 -1.          1.         -1.\n",
      "  1.          1.         -0.94721407]\n",
      "\u001b[96m 164: reward is -1.7055058900044835\u001b[00m\n",
      "[ 1.          0.9824602  -0.898142   -1.         -0.97619873  1.\n",
      " -0.9440044   1.          0.7970327  -1.          1.         -1.\n",
      "  1.          1.         -0.96830505]\n",
      "\u001b[96m 165: reward is -1.7058478800076573\u001b[00m\n",
      "[ 1.          0.9903562  -0.921386   -1.         -0.9966182   1.\n",
      " -0.93309444  1.          0.66704607 -1.          1.         -1.\n",
      "  1.          1.         -0.99031085]\n",
      "\u001b[96m 166: reward is -1.7290989499941398\u001b[00m\n",
      "[ 1.          1.         -0.8993113  -1.         -1.          1.\n",
      " -0.9180371   1.          0.32007852 -1.          1.         -1.\n",
      "  1.          1.         -0.9786248 ]\n",
      "\u001b[96m 167: reward is -1.7085652800007607\u001b[00m\n",
      "[ 1.          0.9683279  -0.90472925 -1.         -1.          1.\n",
      " -0.8994874   1.         -0.26476663 -1.          1.         -1.\n",
      "  1.          1.         -0.9693362 ]\n",
      "\u001b[96m 168: reward is -1.6697712600056176\u001b[00m\n",
      "[ 1.          0.97035867 -0.92640215 -1.         -1.          1.\n",
      " -0.92085886  1.         -0.7321007  -1.          1.         -1.\n",
      "  1.          1.         -0.9608915 ]\n",
      "\u001b[96m 169: reward is -1.6257588100043079\u001b[00m\n",
      "[ 1.          0.96548223 -0.9492209  -1.         -1.          1.\n",
      " -0.9184579   1.         -0.977931   -1.          0.9990401  -1.\n",
      "  1.          1.         -0.9610646 ]\n",
      "\u001b[96m 170: reward is -1.607277679998423\u001b[00m\n",
      "[ 1.          0.9874544  -0.97155845 -1.         -1.          1.\n",
      " -0.91683537  1.         -1.         -1.          0.99721384 -1.\n",
      "  1.          1.         -0.9604302 ]\n",
      "\u001b[96m 171: reward is -1.650637369998251\u001b[00m\n",
      "[ 1.          0.9694714  -0.9860494  -1.         -1.          1.\n",
      " -0.9586336   1.         -1.         -1.          0.9771683  -1.\n",
      "  1.          1.         -0.96569717]\n",
      "\u001b[96m 172: reward is -1.6475451799973986\u001b[00m\n",
      "[ 1.          0.96763587 -1.         -1.         -1.          1.\n",
      " -0.96721125  1.         -1.         -1.          0.9868065  -1.\n",
      "  1.          1.         -0.95978576]\n",
      "\u001b[96m 173: reward is -1.6187221600013901\u001b[00m\n",
      "[ 1.          0.96112984 -0.994991   -1.         -1.          1.\n",
      " -0.925502    1.         -1.         -1.          0.98961854 -1.\n",
      "  1.          1.         -0.9293787 ]\n",
      "\u001b[96m 174: reward is -1.6128964199975715\u001b[00m\n",
      "[ 1.          0.9672687  -0.9782532  -1.         -1.          1.\n",
      " -0.9360839   1.         -1.         -1.          0.99084026 -1.\n",
      "  1.          1.         -0.9225602 ]\n",
      "\u001b[96m 175: reward is -1.65432374999539\u001b[00m\n",
      "[ 1.          0.95070183 -0.9579624  -1.         -1.          1.\n",
      " -0.87120867  1.         -1.         -1.          1.         -1.\n",
      "  1.          1.         -0.9085532 ]\n",
      "\u001b[96m 176: reward is -1.6262993000052335\u001b[00m\n",
      "[ 1.          0.9619563  -0.94857895 -1.         -1.          1.\n",
      " -0.8685009   1.         -1.         -1.          1.         -1.\n",
      "  1.          1.         -0.9758959 ]\n",
      "\u001b[96m 177: reward is -1.6285793300012918\u001b[00m\n",
      "[ 1.         0.950389  -0.9498189 -1.        -0.9949957  1.\n",
      " -0.8802042  1.        -1.        -1.         1.        -1.\n",
      "  1.         1.        -0.9918945]\n",
      "\u001b[96m 178: reward is -1.639502610002018\u001b[00m\n",
      "[ 1.          0.9137156  -0.9672979  -1.         -0.998717    1.\n",
      " -0.90029365  1.         -1.         -1.          1.         -1.\n",
      "  1.          1.         -1.        ]\n",
      "\u001b[96m 179: reward is -1.6391622699960136\u001b[00m\n",
      "[ 1.          0.8886559  -0.9595717  -1.         -1.          1.\n",
      " -0.91485345  1.         -1.         -1.          1.         -1.\n",
      "  1.          1.         -0.9934455 ]\n",
      "\u001b[96m 180: reward is -1.6222675900056842\u001b[00m\n",
      "[ 1.          0.88147146 -0.97925544 -1.         -1.          1.\n",
      " -0.8895092   1.         -1.         -1.          1.         -1.\n",
      "  1.          1.         -0.9799834 ]\n",
      "\u001b[96m 181: reward is -1.6273153299977794\u001b[00m\n",
      "[ 1.          0.8788095  -0.98390883 -1.         -0.97478473  1.\n",
      " -0.88794714  1.         -1.         -1.          1.         -1.\n",
      "  1.          1.         -0.9655727 ]\n",
      "\u001b[96m 182: reward is -1.615978839998315\u001b[00m\n",
      "[ 1.          0.8957713  -1.         -1.         -0.9710685   1.\n",
      " -0.84889925  1.         -1.         -1.          1.         -1.\n",
      "  1.          1.         -0.95984113]\n",
      "\u001b[96m 183: reward is -1.6327654399969942\u001b[00m\n",
      "[ 1.          0.9027572  -1.         -0.9960965  -0.96118987  1.\n",
      " -0.8614035   1.         -1.         -1.          1.         -1.\n",
      "  1.          1.         -0.94567466]\n",
      "\u001b[96m 184: reward is -1.6205749900039192\u001b[00m\n",
      "[ 1.          0.93887913 -1.         -0.81879747 -0.93844646  1.\n",
      " -0.84406483  1.         -1.         -0.984199    1.         -1.\n",
      "  1.          1.         -0.9368206 ]\n",
      "\u001b[96m 185: reward is -1.6580750400046305\u001b[00m\n",
      "[ 1.          0.91497576 -1.         -0.41959566 -0.91935474  1.\n",
      " -0.856462    1.         -1.         -0.8060288   1.         -0.9863859\n",
      "  1.          1.         -0.9443126 ]\n",
      "\u001b[96m 186: reward is -1.6682596700069006\u001b[00m\n",
      "[ 0.98827004  0.8951039  -1.          0.17633085 -0.9058094   1.\n",
      " -0.8584784   1.         -1.         -0.33836675  1.         -0.99715674\n",
      "  1.          1.         -0.95681113]\n",
      "\u001b[96m 187: reward is -1.6374935699974373\u001b[00m\n",
      "[ 0.9839784   0.89336246 -1.          0.6070444  -0.9003154   1.\n",
      " -0.8378809   1.         -1.          0.29625067  1.         -1.\n",
      "  1.          1.         -0.9351645 ]\n",
      "\u001b[96m 188: reward is -1.6654112999933866\u001b[00m\n",
      "[ 0.9525654   0.86911774 -1.          0.76837885 -0.8670774   1.\n",
      " -0.8578789   1.         -1.          0.6824384   1.         -0.9832887\n",
      "  1.          1.         -0.93069786]\n",
      "\u001b[96m 189: reward is -1.690083110005553\u001b[00m\n",
      "[ 0.93763506  0.8681537  -1.          0.8124025  -0.7714797   1.\n",
      " -0.82367575  0.72976303 -1.          0.78307915  1.         -0.96123505\n",
      "  1.          1.         -0.93130505]\n",
      "\u001b[96m 190: reward is -1.702701079995674\u001b[00m\n",
      "[ 0.89408195  0.71373725 -1.          0.83052635 -0.42590082  1.\n",
      " -0.7888458   0.14258724 -1.          0.8606834   1.         -0.9578351\n",
      "  1.          1.         -0.95083535]\n",
      "\u001b[96m 191: reward is -1.6350312000054703\u001b[00m\n",
      "[ 0.78680456  0.37884608 -0.9899356   0.81143355  0.12261353  1.\n",
      " -0.6874042  -0.4013183  -1.          0.8477678   1.         -0.90760815\n",
      "  0.89438087  1.         -0.93539   ]\n",
      "\u001b[96m 192: reward is -1.6667914300066837\u001b[00m\n",
      "[ 0.47944608 -0.17602448 -0.82005304  0.85044897  0.60774815  1.\n",
      " -0.45617756 -0.6661907  -1.          0.8540621   1.         -0.77864605\n",
      "  0.5051018   1.         -0.81494385]\n",
      "\u001b[96m 193: reward is -1.6840167200040166\u001b[00m\n",
      "[-1.4356620e-02 -5.8353949e-01 -5.0267172e-01  8.6894453e-01\n",
      "  8.3011383e-01  1.0000000e+00 -6.0581025e-02 -7.6332331e-01\n",
      " -1.0000000e+00  8.1329060e-01  9.1649437e-01 -5.3146678e-01\n",
      " -2.2839011e-04  1.0000000e+00 -5.3321439e-01]\n",
      "\u001b[96m 194: reward is -1.6776943300037201\u001b[00m\n",
      "[-0.49535182 -0.7855437  -0.02354179  0.8855737   0.93994147  1.\n",
      "  0.3807454  -0.80671036 -1.          0.8193015   0.58945554 -0.1286572\n",
      " -0.41410664  1.         -0.0739945 ]\n",
      "\u001b[96m 195: reward is -1.6732253200079688\u001b[00m\n",
      "[-0.7852284  -0.89900047  0.3794465   0.917633    0.9735153   1.\n",
      "  0.6925247  -0.80053353 -1.          0.8254886   0.18562146  0.25914124\n",
      " -0.6348889   1.          0.35010427]\n",
      "\u001b[96m 196: reward is -1.6463746000043629\u001b[00m\n",
      "[-0.8891746  -0.94026816  0.65671694  0.9262898   0.9894812   1.\n",
      "  0.8632799  -0.80018437 -1.          0.81224227 -0.09714235  0.5831301\n",
      " -0.74683714  1.          0.6232888 ]\n",
      "\u001b[96m 197: reward is -1.6977131399960699\u001b[00m\n",
      "[-0.9875615  -0.9186934   0.7791441   0.9533752   0.9862699   1.\n",
      "  0.9362158  -0.81017697 -1.          0.8034347  -0.36037016  0.77559656\n",
      " -0.77111566  1.          0.7920655 ]\n",
      "\u001b[96m 198: reward is -1.7289810499984188\u001b[00m\n",
      "[-0.99126756 -0.90463305  0.85786724  0.9468139   1.          1.\n",
      "  0.9797247  -0.82013005 -1.          0.7949795  -0.53336775  0.8758487\n",
      " -0.75467116  1.          0.88700354]\n",
      "\u001b[96m 199: reward is -1.8033707999914534\u001b[00m\n",
      "\u001b[94m #3: episode_reward: -83.72424490001885 steps:200\u001b[00m\n",
      "[-0.9714462  -0.99921083  0.9034528   0.9912282   1.          1.\n",
      "  0.9611238  -0.9764753  -0.9871647   0.983836   -0.8542778   0.910495\n",
      " -0.98209494  0.97746265  0.9056042 ]\n",
      "\u001b[96m 200: reward is -1.7447527700131713\u001b[00m\n",
      "[-0.96086025 -0.9899965   0.8963841   1.          1.          1.\n",
      "  0.9903727  -0.9762473  -0.9963633   0.96716523 -0.9006659   0.97733635\n",
      " -1.          0.9754912   0.9573711 ]\n",
      "\u001b[96m 201: reward is -1.7537248900057212\u001b[00m\n",
      "[-1.         -0.992024    0.91007197  1.          0.999965    1.\n",
      "  1.         -1.         -0.9958431   0.9978872  -0.96314895  0.9863549\n",
      " -0.9907576   0.986799    0.992952  ]\n",
      "\u001b[96m 202: reward is -1.720368160012986\u001b[00m\n",
      "[-1.         -1.          0.9649854   1.          1.          1.\n",
      "  1.         -1.         -1.          0.99446213 -1.          0.9859869\n",
      " -0.95283854  0.98065734  0.9745571 ]\n",
      "\u001b[96m 203: reward is -1.7465217299949751\u001b[00m\n",
      "[-1.         -1.          0.98642606  0.97940534  0.998491    1.\n",
      "  1.         -1.         -1.          1.         -1.          0.97576684\n",
      " -0.9575719   1.          0.99566066]\n",
      "\u001b[96m 204: reward is -1.7100747500038427\u001b[00m\n",
      "[-1.         -1.          0.9694092   0.9906205   0.9974118   1.\n",
      "  1.         -1.         -1.          0.98539394 -1.          0.9764058\n",
      " -0.9801915   1.          0.99393517]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\87312\\Documents\\GitHub\\Storm-auto-scheduler\\Simulator\\Spout.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;31m# word_list = np.random.poisson(2.7, size=self.batch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mword_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoisson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[1;31m# word_list = word_list[word_list > 1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0e6829e82497>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     train(args.train_iter, agent, env, None, \n\u001b[0m\u001b[0;32m     26\u001b[0m         args.validate_steps, args.output, max_episode_length=args.max_episode_length, debug=args.debug)\n\u001b[0;32m     27\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-e9c69f6e7c4b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(num_iterations, agent, env, evaluate, validate_steps, output, max_episode_length, debug)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmax_episode_length\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mepisode_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mmax_episode_length\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\87312\\Documents\\GitHub\\Storm-auto-scheduler\\Simulator\\WordCounting.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, new_assignments)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_assignments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_assignments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;31m# the observation is the current deployment(after softmax) + data incoming rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_assignments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\87312\\Documents\\GitHub\\Storm-auto-scheduler\\Simulator\\WordCounting.py\u001b[0m in \u001b[0;36monce\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0monce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwarm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\87312\\Documents\\GitHub\\Storm-auto-scheduler\\Simulator\\Topology.py\u001b[0m in \u001b[0;36mupdate_states\u001b[1;34m(self, time, track)\u001b[0m\n\u001b[0;32m    183\u001b[0m                         f'{len(self.tracking_list)*100/self.tracking_counter:.2f} collected {b_count}')\n\u001b[0;32m    184\u001b[0m                 \u001b[0mnext_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muntil\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m                 \u001b[0mb_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\simpy\\core.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, until)\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopSimulation\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# == until.value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\simpy\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mexc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__cause__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     def run(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}